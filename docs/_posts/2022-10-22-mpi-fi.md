---
layout: single
title:  "intel mpirunがOFI EP enable failedで止まる時の対処療法"
date:   2022-10-20 00:10:00 +0900
categories: linux
tags:
 - linux
 - X11
 - ssh
---

サーバーに新しく追加したマシンでintel mpiに関するエラーが発生したのでその対処法をまとめておく．

## サーバーの環境：Fedora 36

サーバーのLinux環境は以下の通り．
```bash
$ cat /etc/os-release
NAME="Fedora Linux"
VERSION="36 (Server Edition)"
ID=fedora
VERSION_ID=36
VERSION_CODENAME=""
PLATFORM_ID="platform:f36"
PRETTY_NAME="Fedora Linux 36 (Server Edition)"
ANSI_COLOR="0;38;2;60;110;180"
LOGO=fedora-logo-icon
CPE_NAME="cpe:/o:fedoraproject:fedora:36"
HOME_URL="https://fedoraproject.org/"
DOCUMENTATION_URL="https://docs.fedoraproject.org/en-US/fedora/f36/system-administrators-guide/"
SUPPORT_URL="https://ask.fedoraproject.org/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Fedora"
REDHAT_BUGZILLA_PRODUCT_VERSION=36
REDHAT_SUPPORT_PRODUCT="Fedora"
REDHAT_SUPPORT_PRODUCT_VERSION=36
PRIVACY_POLICY_URL="https://fedoraproject.org/wiki/Legal:PrivacyPolicy"
VARIANT="Server Edition"
VARIANT_ID=server
```


## intel mpirun用のテストコマンドとOFIのエラー

intel mpirunは個人的にしょっちゅうエラーが出て困るコマンドの一つ．ノード間通信（ネットワークや証明書）が原因のこともあればintel固有の設定が原因のこともあって対処に苦労している．

今回は

```bash
mpirun -np 8 hostname
```

は問題なく動くが，より複雑な通信を行うコードが動かない場合の対処の一例で，テスト用のコードとして以下のものを使った．

```cc
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    // Initialize the MPI environment
    MPI_Init(NULL, NULL);

    // Get the number of processes
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    // Get the rank of the process
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    // Get the name of the processor
    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_len;
    MPI_Get_processor_name(processor_name, &name_len);

    // Print off a hello world message
    printf("Hello world from processor %s, rank %d out of %d processors\n",
           processor_name, world_rank, world_size);

    // Finalize the MPI environment.
    MPI_Finalize();
}
```

これを`mpiicc`でコンパイル

```
mpiicc test.cc -o test.x
```

して，mpirunで実行すると以下のようなエラーになってしまった．

```
$ mpirun -np 8 ./test.x
Abort(1615247) on node 0 (rank 0 in comm 0): Fatal error in PMPI_Init: Other MPI error, error stack:
MPIR_Init_thread(176)........:
MPID_Init(1538)..............:
MPIDI_OFI_mpi_init_hook(1551):
create_vni_context(2136).....: OFI EP enable failed (ofi_init.c:2136:create_vni_context:Cannot allocate memory)
```

これはMPIのlow level transportを担う`OFI`(Open Fabrics Interfaces)に関するエラーである．


## エラーの対処法

intelの公式のページがかなり参考になる．

https://www.intel.com/content/www/us/en/developer/articles/technical/mpi-library-2019-over-libfabric.html
https://www.intel.com/content/www/us/en/develop/documentation/mpi-developer-guide-linux/top/running-applications/fabrics-control/ofi-providers-support.html
https://www.intel.com/content/www/us/en/develop/documentation/mpi-developer-guide-linux/top/troubleshooting/error-message-fatal-error.html

https://community.intel.com/t5/Intel-oneAPI-HPC-Toolkit/Intel-MPI-update-6-with-MLX-provider-cannot-be-used-without/m-p/1197328?profile.language=ja

まず，mpirunに`I_MPI_DEBUG=4`というオプションをつけて実行するとOFIのライブラリである`libfabric`の情報を表示できる．今回はこれでmpirunが成功するノードと失敗するノードの出力を比較して原因がわかった．

```bash
# 計算が失敗するノード
$ I_MPI_DEBUG=4 mpirun -np 2 ./test.x
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
[0] MPI startup(): Intel(R) MPI Library, Version 2021.7  Build 20220909 (id: 6b6f6425df)
[0] MPI startup(): Copyright (C) 2003-2022 Intel Corporation.  All rights reserved.
[0] MPI startup(): library kind: release
[0] MPI startup(): libfabric version: 1.13.2rc1-impi
[0] MPI startup(): libfabric provider: verbs;ofi_rxm ← ココ！
Abort(1615247) on node 0 (rank 0 in comm 0): Fatal error in PMPI_Init: Other MPI error, error stack:
MPIR_Init_thread(176)........:
MPID_Init(1538)..............:
MPIDI_OFI_mpi_init_hook(1551):
create_vni_context(2136).....: OFI EP enable failed (ofi_init.c:2136:create_vni_context:Cannot allocate memory)

# 計算が成功するノード
$ I_MPI_DEBUG=4 mpirun -np 2 ./test.x
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
[0] MPI startup(): Intel(R) MPI Library, Version 2021.7  Build 20220909 (id: 6b6f6425df)
[0] MPI startup(): Copyright (C) 2003-2022 Intel Corporation.  All rights reserved.
[0] MPI startup(): library kind: release
[0] MPI startup(): libfabric version: 1.13.2rc1-impi
[0] MPI startup(): libfabric provider: tcp;ofi_rxm  ← ココ！
[0] MPI startup(): File "/opt/intel/oneapi/mpi/2021.7.0/etc/tuning_skx_shm-ofi_tcp-ofi-rxm_1.dat" not found
[0] MPI startup(): Load tuning file: "/opt/intel/oneapi/mpi/2021.7.0/etc/tuning_skx_shm-ofi_tcp-ofi-rxm.dat"
Hello world from processor hoge, rank 1 out of 2 processors
[0] MPI startup(): Rank    Pid      Node name    Pin cpu
[0] MPI startup(): 0       250790   hoge         {0,1,2,3}
[0] MPI startup(): 1       250791   hoge         {4,5,6,7}
Hello world from processor hoge, rank 0 out of 2 processors
```

成功したノードと失敗したノードでは`libfabric provider`が異なる．

```
MPI startup(): libfabric provider: tcp;ofi_rxm  
MPI startup(): libfabric provider: verbs;ofi_rxm
```

どのマシンにも同じ設定をしているはずなのだがなぜかデフォルトの状態が異なっている．．．そちらの問題はさておいてlibfabric providerを変更したければ`FI_PROVIDER`または`I_MPI_OFI_PROVIDER`環境変数を設定すればよい．（上記ページによると後者が推奨らしい）

```
export I_MPI_OFI_PROVIDER=tcp
```
lmodによるmoduleシステムを採用した環境では，intel mpiのモジュール設定ファイルにこの環境変数設定を追記しておけば全てのユーザーに対してこの問題を解決できる．

## その他

OFI関連の環境変数としては`I_MPI_OFI_LIBRARY_INTERNAL`や`FI_PROVIDER_PATH`もあるけど今回はそっちは大丈夫そうだった．

